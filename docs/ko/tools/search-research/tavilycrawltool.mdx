---
title: "Tavily 크롤 도구"
description: "Tavily API를 사용하여 웹사이트를 크롤링하고 여러 페이지에서 콘텐츠 추출"
icon: "spider"
mode: "wide"
---

`TavilyCrawlTool`은 CrewAI 에이전트가 Tavily API를 사용하여 기본 URL에서 시작하여 웹사이트를 크롤링할 수 있도록 합니다. 링크를 지능적으로 탐색하고 여러 페이지에서 대규모로 구조화된 콘텐츠를 추출합니다.

## 설치

`TavilyCrawlTool`을 사용하려면 `tavily-python` 라이브러리를 설치해야 합니다:

```shell
pip install 'crewai[tools]' tavily-python
```

또한 환경 변수로 Tavily API 키를 설정해야 합니다:

```bash
export TAVILY_API_KEY='your-tavily-api-key'
```

https://app.tavily.com/에서 API 키를 발급받으세요(회원가입 후 키를 생성하면 됩니다).

## 예제 사용법

다음은 CrewAI 에이전트에서 `TavilyCrawlTool`을 초기화하고 사용하는 방법입니다:

```python
import os
from crewai import Agent, Task, Crew
from crewai_tools import TavilyCrawlTool

# Ensure TAVILY_API_KEY is set in your environment
# os.environ["TAVILY_API_KEY"] = "YOUR_API_KEY"

# Initialize the tool
crawl_tool = TavilyCrawlTool()

# Create an agent that uses the tool
crawler_agent = Agent(
    role='Web Crawler',
    goal='Crawl websites and extract comprehensive content',
    backstory='You are an expert at crawling websites and extracting relevant content using the Tavily API.',
    tools=[crawl_tool],
    verbose=True
)

# Define a task for the agent
crawl_task = Task(
    description='Crawl the documentation at https://docs.example.com and extract all relevant content.',
    expected_output='A JSON string containing the crawled content from the website.',
    agent=crawler_agent
)

# Create and run the crew
crew = Crew(
    agents=[crawler_agent],
    tasks=[crawl_task],
    verbose=2
)

result = crew.kickoff()
print(result)
```

## 구성 옵션

### 에이전트 설정 가능 매개변수 (런타임)

이 매개변수들은 에이전트가 런타임에 제공할 수 있습니다:

- `url` (str): **필수**. 크롤링을 시작할 기본 URL.
- `max_depth` (int, 1-5, 선택): 크롤링할 최대 깊이 (기본 URL에서의 링크 홉 수).
- `extract_depth` (Literal["basic", "advanced"], 선택): 페이지 콘텐츠 추출 깊이 - 'basic'은 주요 콘텐츠, 'advanced'는 종합적인 추출.
- `instructions` (str, 선택): 크롤러를 안내하는 자연어 지시사항 (예: 'only crawl blog posts', 'focus on product pages').
- `allow_external` (bool, 선택): 외부 도메인 크롤링을 허용할지 여부.

### 사용자 설정 가능 매개변수 (초기화)

이 매개변수들은 도구를 생성할 때 구성됩니다:

- `max_breadth` (int, 선택): 페이지당 팔로우할 최대 링크 수.
- `limit` (int, 선택): 중지하기 전에 처리할 총 링크 수.
- `select_paths` (Sequence[str], 선택): 특정 경로 패턴을 가진 URL만 선택하는 정규식 패턴 (예: `/docs/.*`, `/api/.*`).
- `select_domains` (Sequence[str], 선택): 특정 도메인 또는 서브도메인에 크롤링을 선택하는 정규식 패턴.
- `exclude_paths` (Sequence[str], 선택): 특정 경로 패턴을 가진 URL을 제외하는 정규식 패턴 (예: `/private/.*`, `/admin/.*`).
- `exclude_domains` (Sequence[str], 선택): 특정 도메인 또는 서브도메인을 제외하는 정규식 패턴.
- `include_images` (bool, 선택): 크롤 결과에 이미지를 포함할지 여부.
- `format` (Literal["markdown", "text"], 선택): 추출된 웹 페이지 콘텐츠의 형식.
- `timeout` (float, 선택): 크롤 요청의 타임아웃(초). 기본값은 `150`.
- `include_favicon` (bool, 선택): 크롤 결과에 파비콘 URL을 포함할지 여부.
- `include_usage` (bool, 선택): 응답에 크레딧 사용 정보를 포함할지 여부.
- `chunks_per_source` (int, 1-5, 선택): 소스당 최대 콘텐츠 청크 수. 지시사항이 제공될 때만 사용됨.
- `extra_kwargs` (dict, 선택): tavily-python에 전달할 추가 키워드 인자.

## 고급 사용법

### 경로 필터를 사용한 크롤링

```python
# Configure the tool to crawl only documentation pages
custom_crawler = TavilyCrawlTool(
    max_breadth=10,
    limit=100,
    select_paths=[r'/docs/.*', r'/api/.*'],
    exclude_paths=[r'/private/.*', r'/admin/.*'],
    include_images=True,
    format='markdown',
    timeout=120
)

agent_with_custom_tool = Agent(
    role="Documentation Crawler",
    goal="Crawl and extract documentation content",
    tools=[custom_crawler]
)
```

### 자연어 지시사항 사용

에이전트는 크롤러를 안내하는 지시사항을 제공할 수 있습니다:

```python
# Task with specific crawling instructions
crawl_task = Task(
    description='''Crawl https://example.com with instructions to focus on product pages 
    and blog posts. Use max_depth of 3 and allow external links.''',
    expected_output='A JSON string containing the crawled content.',
    agent=crawler_agent
)
```

## 기능

- **지능적 크롤링**: 기본 URL에서 시작하여 자동으로 링크 탐색
- **깊이 제어**: 크롤러가 탐색할 깊이 구성
- **너비 제어**: 페이지당 팔로우하는 링크 수 제한
- **경로 필터링**: 경로 패턴에 따라 URL 선택 또는 제외
- **도메인 필터링**: 크롤링할 수 있는 도메인 제어
- **자연어 지시사항**: 일반 영어로 크롤러 안내
- **콘텐츠 추출**: 발견된 각 페이지에서 구조화된 콘텐츠 추출
- **설정 가능한 출력 형식**: 마크다운 또는 텍스트 형식으로 콘텐츠 획득

## 응답 형식

도구는 웹사이트에서 크롤링된 데이터를 포함하는 JSON 문자열을 반환합니다. 응답에는 다음이 포함됩니다:
- URL이 포함된 크롤링된 페이지 목록
- 각 페이지에서 추출된 콘텐츠
- 이미지 (`include_images=True`인 경우)
- 각 페이지의 메타데이터

응답 구조에 대한 자세한 정보는 [Tavily API 문서](https://docs.tavily.com/documentation/api-reference/endpoint/crawl)를 참조하세요.

## 사용 사례

- **문서 집계**: RAG 파이프라인을 위한 전체 문서 사이트 크롤링
- **콘텐츠 마이그레이션**: 마이그레이션을 위해 여러 페이지에서 콘텐츠 추출
- **경쟁 분석**: 콘텐츠 분석을 위해 경쟁사 웹사이트 크롤링
- **연구**: 웹사이트에서 종합적인 정보 수집
- **아카이빙**: 웹사이트 콘텐츠의 구조화된 백업 생성

