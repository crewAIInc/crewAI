---
title: 메모리
description: CrewAI의 통합 메모리 시스템을 활용하여 에이전트 역량을 강화합니다.
icon: database
mode: "wide"
---

## 개요

CrewAI는 단기, 장기, 엔터티, 외부 메모리를 하나의 지능형 API로 대체하는 **통합 메모리 시스템**을 제공합니다. 메모리는 저장 시 콘텐츠를 분석하고(범위, 카테고리, 중요도 추론) 적응형 깊이 recall을 지원하는 LLM을 사용합니다. 독립 실행, crew, 에이전트, Flow 내부에서 사용할 수 있습니다.

## 빠른 시작

```python
from crewai import Memory

memory = Memory()

# 저장 — scope와 importance를 지정하지 않으면 LLM이 추론
memory.remember("We decided to use PostgreSQL for the user database.")

# 검색
matches = memory.recall("What database did we choose?")
for m in matches:
    print(m.record.content, m.score)

# 삭제
memory.forget(scope="/project/old")

# 탐색
print(memory.tree())
print(memory.info("/"))
```

## Crew와 함께 사용

단일 플래그로 crew에 메모리를 활성화하거나 사용자 정의 인스턴스를 전달합니다:

```python
from crewai import Crew, Agent, Task, Process, Memory

# 옵션 1: 기본 메모리 (기본 LLM 및 LanceDB로 Memory() 생성)
crew = Crew(
    agents=[...],
    tasks=[...],
    process=Process.sequential,
    memory=True,
    verbose=True,
)

# 옵션 2: 사용자 정의 메모리 인스턴스 (예: 저장 경로 또는 LLM)
memory = Memory(storage="./my_memory_db", llm="gpt-4o-mini")
crew = Crew(
    agents=[...],
    tasks=[...],
    process=Process.sequential,
    memory_instance=memory,
    verbose=True,
)
```

`memory=True`로 기본 `Memory()`를 생성할 때 crew의 `embedder` 설정이 전달되므로, LLM 제공자와 맞추거나 로컬 임베딩을 사용할 수 있습니다.

## 에이전트와 함께 사용

에이전트는 crew의 공유 메모리 또는 범위가 지정된 뷰를 사용할 수 있습니다:

```python
from crewai import Agent, Memory

memory = Memory()
crew_memory = memory.scope("/crew/project-alpha")

# 자체 범위 메모리를 가진 에이전트 (/agent/researcher 및 하위만 표시)
researcher = Agent(
    role="Researcher",
    goal="Research topics",
    backstory="Expert researcher",
    memory=memory.scope("/agent/researcher"),
)

# crew 메모리를 사용하는 에이전트 (공유)
writer = Agent(
    role="Writer",
    goal="Write content",
    backstory="Expert writer",
    # memory 미설정 — crew에 memory=True일 때 crew._memory 사용
)
```

## Flow와 함께 사용

모든 Flow에는 `memory` 속성이 있습니다(설정하지 않으면 자동 생성). Flow 메서드 내부에서 사용합니다:

```python
from crewai.flow import Flow

class ResearchFlow(Flow):
    @start()
    def research(self):
        results = self.do_research()
        self.remember(f"Research found: {results}", scope="/research")
        return results

    @listen(research)
    def analyze(self, research_results):
        past = self.recall("previous research findings")
        # 과거 메모리로 맥락화
        return self.synthesize(research_results, past)
```

## 계층적 범위(Scopes)

메모리는 범위 트리(예: `/`, `/project`, `/project/alpha`)로 구성됩니다. `remember()`에 scope를 전달하지 않으면 LLM이 콘텐츠와 기존 scope를 바탕으로 제안합니다.

```python
memory = Memory()

# 모든 연산을 하위 트리로 제한
agent_memory = memory.scope("/agent/researcher")
agent_memory.remember("Found source X.")  # /agent/researcher 아래에 저장
agent_memory.recall("sources")           # 해당 경로에서만 검색

# 더 좁은 하위 scope
project_memory = agent_memory.subscope("project-alpha")
```

## 메모리 슬라이스

슬라이스는 여러 scope에 대한 뷰입니다. 여러 브랜치(예: 에이전트 scope + 회사 지식)에 대한 읽기 전용 접근에 유용합니다:

```python
# 읽기 전용: /agent/researcher와 /company/knowledge에서 recall; remember()는 예외
view = memory.slice(
    scopes=["/agent/researcher", "/company/knowledge"],
    read_only=True,
)
matches = view.recall("company policy on X", limit=10)

# 읽기/쓰기: remember()에 명시적 scope 전달 필요
view_rw = memory.slice(scopes=["/a", "/b"], read_only=False)
view_rw.remember("New fact", scope="/a", categories=[], importance=0.5)
```

## LLM 분석 레이어

메모리는 LLM을 세 가지 방식으로 사용합니다:

1. **저장 시** — scope, categories, importance를 생략하면 LLM이 콘텐츠를 분석하여 scope, categories, importance, 메타데이터(엔터티, 날짜, 주제)를 제안합니다.
2. **recall 시** — deep/auto recall의 경우 LLM이 쿼리(키워드, 시간 힌트, 제안 scope, 복잡도)를 분석하여 검색을 안내합니다.
3. **메모리 추출** — `extract_memories(content)`는 원시 텍스트(예: 작업 + 결과)를 개별 메모리 문장으로 나눕니다. 에이전트는 각 문장에 `remember()`를 호출하기 전에 이를 사용하여 하나의 큰 블록 대신 원자적 사실이 저장되도록 합니다.

## RecallFlow (딥 Recall)

`recall()`은 세 가지 깊이를 지원합니다:

- **`depth="shallow"`** — 복합 점수(의미 + 최신성 + 중요도)를 사용한 직접 벡터 검색. 빠름; 에이전트가 작업용 컨텍스트를 로드할 때 기본 사용.
- **`depth="deep"`** 또는 **`depth="auto"`** — 다단계 RecallFlow 실행: 쿼리 분석, scope 선택, 벡터 검색, 신뢰도 기반 라우팅, 신뢰도가 낮을 때 선택적 재귀 탐색.

```python
# 빠른 경로 (에이전트 작업 컨텍스트 기본값)
matches = memory.recall("What did we decide?", limit=10, depth="shallow")

# 복잡한 질문용 지능형 경로
matches = memory.recall("Summarize all architecture decisions", limit=10, depth="auto")
```

## 복합 점수(Composite Scoring)

관련성은 의미 유사도, 최신성, 중요도의 가중 조합입니다. `MemoryConfig`로 가중치를 설정합니다:

```python
from crewai.memory import Memory, MemoryConfig

config = MemoryConfig(
    recency_weight=0.3,
    semantic_weight=0.5,
    importance_weight=0.2,
    recency_half_life_days=30,
)
memory = Memory(config=config)
```

## 탐색(Discovery)

scope 계층 구조와 카테고리를 검사합니다:

```python
memory.tree()                    # scope 및 레코드 수의 포맷된 트리
memory.tree("/project", max_depth=2)
memory.info("/project")         # ScopeInfo: record_count, categories, oldest/newest
memory.list_scopes("/")          # 직계 자식 scope
memory.list_categories()         # 카테고리 이름 및 개수(전역 또는 scope별)
```

## 사용자 정의 Embedder 설정

Crew는 `memory=True`일 때 기본 Memory에 전달되는 `embedder` 설정을 받습니다:

```python
from crewai import Crew

# OpenAI (기본)
crew = Crew(
    agents=[...],
    tasks=[...],
    memory=True,
    embedder={
        "provider": "openai",
        "config": {"model": "text-embedding-3-small"},
    },
)

# Ollama (로컬, 비공개)
crew = Crew(
    agents=[...],
    tasks=[...],
    memory=True,
    embedder={
        "provider": "ollama",
        "config": {"model": "mxbai-embed-large"},
    },
)
```

독립 실행 Memory는 embedder 인스턴스를 받을 수 있습니다:

```python
from crewai.rag.embeddings.factory import build_embedder
from crewai import Memory

embedder = build_embedder({"provider": "openai", "config": {}})
memory = Memory(embedder=embedder)
```

| Provider       | Best For                       | Notes                    |
| -------------- | ------------------------------ | ------------------------- |
| **OpenAI**     | General use                    | Default, high quality     |
| **Ollama**     | Privacy, cost savings          | Local, no API key         |
| **Google AI**  | Google ecosystem               | gemini-embedding-001      |
| **Azure OpenAI** | Enterprise                   | Same config shape as OpenAI |
| **Cohere**     | Multilingual                   | embed-english-v3.0        |
| **VoyageAI**   | Retrieval tasks                | voyage-3                  |
| **Bedrock**    | AWS                            | amazon.titan-embed-text   |
| **Hugging Face** | Open-source models           | sentence-transformers     |

## 스토리지 백엔드

- **기본값**: LanceDB, `./.crewai/memory` 아래에 저장(또는 `storage="path/to/dir"`로 전달한 경로).
- **사용자 정의 백엔드**: `StorageBackend` 프로토콜 구현(참조: `crewai.memory.storage.backend`) 후 `Memory(storage=your_backend)`에 인스턴스 전달.

## 오류 시 동작

분석 중 LLM이 실패하면(네트워크 오류, 속도 제한, 잘못된 응답) 메모리는 우아하게 저하됩니다:

- **저장 분석** — 경고가 로깅되고 메모리는 기본 scope `/`, 빈 categories, importance `0.5`로 저장됩니다.
- **메모리 추출** — 전체 콘텐츠가 단일 메모리로 저장되어 누락되지 않습니다.
- **쿼리 분석** — recall은 단순 scope 선택 및 벡터 검색으로 폴백하여 결과를 계속 반환합니다.

이러한 분석 실패에서는 예외가 발생하지 않으며, 스토리지 또는 embedder 실패만 예외를 발생시킵니다.

## 개인정보 참고

메모리 콘텐츠는 분석을 위해 설정된 LLM으로 전송됩니다(저장 시 scope/categories/importance, 쿼리 분석 및 선택적 딥 recall). 민감한 데이터의 경우 로컬 LLM(예: Ollama)을 사용하거나 제공자가 규정 요구 사항을 충족하는지 확인하세요.

## 메모리 이벤트

모든 메모리 연산은 `source_type="unified_memory"`로 이벤트를 발생시킵니다. 시간, 오류, 콘텐츠를 수신할 수 있습니다.

| Event | Description | Key Properties |
| :---- | :---------- | :------------- |
| **MemoryQueryStartedEvent** | Query begins | `query`, `limit` |
| **MemoryQueryCompletedEvent** | Query succeeds | `query`, `results`, `query_time_ms` |
| **MemoryQueryFailedEvent** | Query fails | `query`, `error` |
| **MemorySaveStartedEvent** | Save begins | `value`, `metadata` |
| **MemorySaveCompletedEvent** | Save succeeds | `value`, `save_time_ms` |
| **MemorySaveFailedEvent** | Save fails | `value`, `error` |
| **MemoryRetrievalStartedEvent** | Agent retrieval starts | `task_id` |
| **MemoryRetrievalCompletedEvent** | Agent retrieval done | `task_id`, `memory_content`, `retrieval_time_ms` |

예: 쿼리 시간 모니터링:

```python
from crewai.events import BaseEventListener, MemoryQueryCompletedEvent

class MemoryMonitor(BaseEventListener):
    def setup_listeners(self, crewai_event_bus):
        @crewai_event_bus.on(MemoryQueryCompletedEvent)
        def on_done(source, event):
            if getattr(event, "source_type", None) == "unified_memory":
                print(f"Query '{event.query}' completed in {event.query_time_ms:.0f}ms")
```

## 문제 해결

**메모리가 유지되지 않나요?**
- 저장 경로에 쓰기 권한이 있는지 확인하세요(기본값 `./.crewai/memory`). 다른 디렉터리를 사용하려면 `storage="./your_path"`를 전달하세요.
- crew 사용 시 `memory=True` 또는 `memory_instance=...`가 설정되었는지 확인하세요.

**recall이 느린가요?**
- 일상적인 에이전트 컨텍스트에는 `depth="shallow"`를 사용하세요. 복잡한 쿼리에만 `depth="auto"` 또는 `"deep"`을 사용하세요.

**로그에 LLM 분석 오류가 있나요?**
- 메모리는 안전한 기본값으로 계속 저장/recall합니다. 전체 LLM 분석을 원하면 API 키, 속도 제한, 모델 가용성을 확인하세요.

**메모리 초기화(예: 테스트용):**
```python
crew.reset_memories(command_type="memory")  # 통합 메모리 초기화
# 또는 Memory 인스턴스에서:
memory.reset()           # 모든 scope
memory.reset(scope="/project/old")  # 해당 하위 트리만
```

## 장점

- **단일 API** — 모든 사용 사례에 하나의 `Memory` 클래스; short/long/entity/external 타입 분리 없음.
- **LLM 기반 구성** — 콘텐츠에서 scope와 categories 추론; 더 깔끔한 recall을 위한 선택적 원자 추출.
- **적응형 recall** — 속도를 위한 shallow 경로, 신뢰도 기반 라우팅이 있는 복잡한 질문용 deep 경로.
- **구성 가능한 뷰** — 스토리지 분할 없이 에이전트별 또는 다중 scope 접근을 위한 scope와 slice.
- **관찰 가능** — 모든 저장 및 쿼리에 대한 이벤트로 모니터링 및 디버깅 가능.
