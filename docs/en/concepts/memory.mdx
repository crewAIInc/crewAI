---
title: Memory
description: Leveraging the unified memory system in CrewAI to enhance agent capabilities.
icon: database
mode: "wide"
---

## Overview

CrewAI provides a **unified memory system** that replaces separate short-term, long-term, entity, and external memory with a single intelligent API. Memory uses an LLM to analyze content when saving (inferring scope, categories, and importance) and supports adaptive-depth recall. You can use it standalone, with crews, with agents, or inside Flows.

## Quick Start

```python
from crewai import Memory

memory = Memory()

# Store — the LLM infers scope and importance when not provided
memory.remember("We decided to use PostgreSQL for the user database.")

# Retrieve
matches = memory.recall("What database did we choose?")
for m in matches:
    print(m.record.content, m.score)

# Forget
memory.forget(scope="/project/old")

# Explore
print(memory.tree())
print(memory.info("/"))
```

## Using with Crews

Enable memory for a crew with a single flag or pass a custom instance:

```python
from crewai import Crew, Agent, Task, Process, Memory

# Option 1: Default memory (creates Memory() with default LLM and LanceDB)
crew = Crew(
    agents=[...],
    tasks=[...],
    process=Process.sequential,
    memory=True,
    verbose=True,
)

# Option 2: Custom memory instance (e.g. custom storage path or LLM)
memory = Memory(storage="./my_memory_db", llm="gpt-4o-mini")
crew = Crew(
    agents=[...],
    tasks=[...],
    process=Process.sequential,
    memory_instance=memory,
    verbose=True,
)
```

The crew's `embedder` configuration is passed through when `memory=True` creates the default `Memory()`, so you can match your LLM provider or use local embeddings.

## Using with Agents

Agents can use the crew's shared memory or a scoped view:

```python
from crewai import Agent, Memory

memory = Memory()
crew_memory = memory.scope("/crew/project-alpha")

# Agent with its own scoped memory (only sees /agent/researcher and below)
researcher = Agent(
    role="Researcher",
    goal="Research topics",
    backstory="Expert researcher",
    memory=memory.scope("/agent/researcher"),
)

# Agent using crew memory (shared)
writer = Agent(
    role="Writer",
    goal="Write content",
    backstory="Expert writer",
    # memory not set — uses crew._memory when crew has memory=True
)
```

## Using with Flows

Every Flow has a `memory` attribute (auto-created if not set). Use it inside flow methods:

```python
from crewai.flow import Flow

class ResearchFlow(Flow):
    @start()
    def research(self):
        results = self.do_research()
        self.remember(f"Research found: {results}", scope="/research")
        return results

    @listen(research)
    def analyze(self, research_results):
        past = self.recall("previous research findings")
        # Use past memories to contextualize
        return self.synthesize(research_results, past)
```

## Hierarchical Scopes

Memories are organized in a scope tree (e.g. `/`, `/project`, `/project/alpha`). When you don't pass a scope to `remember()`, the LLM suggests one based on content and existing scopes.

```python
memory = Memory()

# Restrict all operations to a subtree
agent_memory = memory.scope("/agent/researcher")
agent_memory.remember("Found source X.")  # Stored under /agent/researcher
agent_memory.recall("sources")           # Searches only under that path

# Narrower subscope
project_memory = agent_memory.subscope("project-alpha")
```

## Memory Slices

A slice is a view over multiple scopes. Useful for read-only access to several branches (e.g. agent scope + company knowledge):

```python
# Read-only: recall across /agent/researcher and /company/knowledge; remember() raises
view = memory.slice(
    scopes=["/agent/researcher", "/company/knowledge"],
    read_only=True,
)
matches = view.recall("company policy on X", limit=10)

# Read-write: must pass explicit scope to remember()
view_rw = memory.slice(scopes=["/a", "/b"], read_only=False)
view_rw.remember("New fact", scope="/a", categories=[], importance=0.5)
```

## LLM Analysis Layer

Memory uses the LLM in three ways:

1. **On save** — When you omit scope, categories, or importance, the LLM analyzes the content and suggests scope, categories, importance, and metadata (entities, dates, topics).
2. **On recall** — For deep/auto recall, the LLM analyzes the query (keywords, time hints, suggested scopes, complexity) to guide retrieval.
3. **Extract memories** — `extract_memories(content)` breaks raw text (e.g. task + result) into discrete memory statements. Agents use this before calling `remember()` on each statement so that atomic facts are stored instead of one large blob.

## RecallFlow (Deep Recall)

`recall()` supports three depths:

- **`depth="shallow"`** — Direct vector search with composite scoring (semantic + recency + importance). Fast; used by default when agents load context for a task.
- **`depth="deep"`** or **`depth="auto"`** — Runs a multi-step RecallFlow: query analysis, scope selection, vector search, confidence-based routing, and optional recursive exploration when confidence is low.

```python
# Fast path (default for agent task context)
matches = memory.recall("What did we decide?", limit=10, depth="shallow")

# Intelligent path for complex questions
matches = memory.recall("Summarize all architecture decisions", limit=10, depth="auto")
```

## Composite Scoring

Relevance is a weighted combination of semantic similarity, recency, and importance. Configure weights via `MemoryConfig`:

```python
from crewai.memory import Memory, MemoryConfig

config = MemoryConfig(
    recency_weight=0.3,
    semantic_weight=0.5,
    importance_weight=0.2,
    recency_half_life_days=30,
)
memory = Memory(config=config)
```

## Discovery

Inspect scope hierarchy and categories:

```python
memory.tree()                    # Formatted tree of scopes and record counts
memory.tree("/project", max_depth=2)
memory.info("/project")         # ScopeInfo: record_count, categories, oldest/newest
memory.list_scopes("/")          # Immediate child scopes
memory.list_categories()         # Category names and counts (global or per scope)
```

## Custom Embedder Configuration

Crews accept an `embedder` config that is passed to the default Memory when `memory=True`:

```python
from crewai import Crew

# OpenAI (default)
crew = Crew(
    agents=[...],
    tasks=[...],
    memory=True,
    embedder={
        "provider": "openai",
        "config": {"model": "text-embedding-3-small"},
    },
)

# Ollama (local, private)
crew = Crew(
    agents=[...],
    tasks=[...],
    memory=True,
    embedder={
        "provider": "ollama",
        "config": {"model": "mxbai-embed-large"},
    },
)
```

Standalone Memory can take an embedder instance:

```python
from crewai.rag.embeddings.factory import build_embedder
from crewai import Memory

embedder = build_embedder({"provider": "openai", "config": {}})
memory = Memory(embedder=embedder)
```

| Provider       | Best For                       | Notes                    |
| -------------- | ------------------------------ | ------------------------- |
| **OpenAI**     | General use                    | Default, high quality     |
| **Ollama**     | Privacy, cost savings          | Local, no API key         |
| **Google AI**  | Google ecosystem               | gemini-embedding-001      |
| **Azure OpenAI** | Enterprise                   | Same config shape as OpenAI |
| **Cohere**     | Multilingual                   | embed-english-v3.0        |
| **VoyageAI**   | Retrieval tasks                | voyage-3                  |
| **Bedrock**    | AWS                            | amazon.titan-embed-text   |
| **Hugging Face** | Open-source models           | sentence-transformers     |

## Storage Backend

- **Default**: LanceDB, stored under `./.crewai/memory` (or the path you pass as `storage="path/to/dir"`).
- **Custom backend**: Implement the `StorageBackend` protocol (see `crewai.memory.storage.backend`) and pass an instance to `Memory(storage=your_backend)`.

## Failure Behavior

If the LLM fails during analysis (network error, rate limit, invalid response), memory degrades gracefully:

- **Save analysis** — A warning is logged and the memory is still stored with default scope `/`, empty categories, and importance `0.5`.
- **Extract memories** — The full content is stored as a single memory so nothing is dropped.
- **Query analysis** — Recall falls back to simple scope selection and vector search so you still get results.

No exception is raised for these analysis failures; only storage or embedder failures will raise.

## Privacy Note

Memory content is sent to the configured LLM for analysis (scope/categories/importance on save, query analysis and optional deep recall). For sensitive data, use a local LLM (e.g. Ollama) or ensure your provider meets your compliance requirements.

## Memory Events

All memory operations emit events with `source_type="unified_memory"`. You can listen for timing, errors, and content.

| Event | Description | Key Properties |
| :---- | :---------- | :------------- |
| **MemoryQueryStartedEvent** | Query begins | `query`, `limit` |
| **MemoryQueryCompletedEvent** | Query succeeds | `query`, `results`, `query_time_ms` |
| **MemoryQueryFailedEvent** | Query fails | `query`, `error` |
| **MemorySaveStartedEvent** | Save begins | `value`, `metadata` |
| **MemorySaveCompletedEvent** | Save succeeds | `value`, `save_time_ms` |
| **MemorySaveFailedEvent** | Save fails | `value`, `error` |
| **MemoryRetrievalStartedEvent** | Agent retrieval starts | `task_id` |
| **MemoryRetrievalCompletedEvent** | Agent retrieval done | `task_id`, `memory_content`, `retrieval_time_ms` |

Example: monitor query time:

```python
from crewai.events import BaseEventListener, MemoryQueryCompletedEvent

class MemoryMonitor(BaseEventListener):
    def setup_listeners(self, crewai_event_bus):
        @crewai_event_bus.on(MemoryQueryCompletedEvent)
        def on_done(source, event):
            if getattr(event, "source_type", None) == "unified_memory":
                print(f"Query '{event.query}' completed in {event.query_time_ms:.0f}ms")
```

## Troubleshooting

**Memory not persisting?**
- Ensure the storage path is writable (default `./.crewai/memory`). Pass `storage="./your_path"` to use a different directory.
- When using a crew, confirm `memory=True` or `memory_instance=...` is set.

**Slow recall?**
- Use `depth="shallow"` for routine agent context. Reserve `depth="auto"` or `"deep"` for complex queries.

**LLM analysis errors in logs?**
- Memory still saves/recalls with safe defaults. Check API keys, rate limits, and model availability if you want full LLM analysis.

**Reset memory (e.g. for tests):**
```python
crew.reset_memories(command_type="memory")  # Resets unified memory
# Or on a Memory instance:
memory.reset()           # All scopes
memory.reset(scope="/project/old")  # Only that subtree
```

## Benefits

- **Single API** — One `Memory` class for all use cases; no separate short/long/entity/external types.
- **LLM-organized** — Scope and categories inferred from content; optional atomic extraction for cleaner recall.
- **Adaptive recall** — Shallow path for speed, deep path for complex questions with confidence-based routing.
- **Composable views** — Scopes and slices for per-agent or multi-scope access without fragmenting storage.
- **Observable** — Events for every save and query so you can monitor and debug.
