---
title: Memória
description: Aproveitando o sistema de memória unificado no CrewAI para aprimorar as capacidades dos agentes.
icon: database
mode: "wide"
---

## Visão Geral

O CrewAI oferece um **sistema de memória unificado** que substitui memória de curto prazo, longo prazo, entidades e externa por uma única API inteligente. A memória usa um LLM para analisar o conteúdo ao salvar (inferindo escopo, categorias e importância) e suporta recall com profundidade adaptativa. Você pode usá-la de forma independente, com crews, com agentes ou dentro de Flows.

## Início Rápido

```python
from crewai import Memory

memory = Memory()

# Armazenar — o LLM infere escopo e importância quando não fornecidos
memory.remember("Decidimos usar PostgreSQL para o banco de dados do usuário.")

# Recuperar
matches = memory.recall("Qual banco de dados escolhemos?")
for m in matches:
    print(m.record.content, m.score)

# Esquecer
memory.forget(scope="/project/old")

# Explorar
print(memory.tree())
print(memory.info("/"))
```

## Uso com Crews

Ative a memória para uma crew com um único flag ou passe uma instância customizada:

```python
from crewai import Crew, Agent, Task, Process, Memory

# Opção 1: Memória padrão (cria Memory() com LLM e LanceDB padrão)
crew = Crew(
    agents=[...],
    tasks=[...],
    process=Process.sequential,
    memory=True,
    verbose=True,
)

# Opção 2: Instância de memória customizada (ex.: caminho de armazenamento ou LLM)
memory = Memory(storage="./my_memory_db", llm="gpt-4o-mini")
crew = Crew(
    agents=[...],
    tasks=[...],
    process=Process.sequential,
    memory_instance=memory,
    verbose=True,
)
```

A configuração `embedder` da crew é repassada quando `memory=True` cria o `Memory()` padrão, para você combinar com seu provedor de LLM ou usar embeddings locais.

## Uso com Agentes

Agentes podem usar a memória compartilhada da crew ou uma visão com escopo:

```python
from crewai import Agent, Memory

memory = Memory()
crew_memory = memory.scope("/crew/project-alpha")

# Agente com memória própria (vê apenas /agent/researcher e abaixo)
researcher = Agent(
    role="Researcher",
    goal="Research topics",
    backstory="Expert researcher",
    memory=memory.scope("/agent/researcher"),
)

# Agente usando memória da crew (compartilhada)
writer = Agent(
    role="Writer",
    goal="Write content",
    backstory="Expert writer",
    # memory não definido — usa crew._memory quando crew tem memory=True
)
```

## Uso com Flows

Todo Flow tem um atributo `memory` (criado automaticamente se não definido). Use-o nos métodos do flow:

```python
from crewai.flow import Flow

class ResearchFlow(Flow):
    @start()
    def research(self):
        results = self.do_research()
        self.remember(f"Pesquisa encontrou: {results}", scope="/research")
        return results

    @listen(research)
    def analyze(self, research_results):
        past = self.recall("resultados de pesquisas anteriores")
        # Use memórias passadas para contextualizar
        return self.synthesize(research_results, past)
```

## Escopos Hierárquicos

As memórias são organizadas em uma árvore de escopos (ex.: `/`, `/project`, `/project/alpha`). Quando você não passa um escopo para `remember()`, o LLM sugere um com base no conteúdo e nos escopos existentes.

```python
memory = Memory()

# Restringir todas as operações a uma subárvore
agent_memory = memory.scope("/agent/researcher")
agent_memory.remember("Encontrada fonte X.")  # Armazenado em /agent/researcher
agent_memory.recall("sources")                 # Busca apenas nesse caminho

# Subescopo mais restrito
project_memory = agent_memory.subscope("project-alpha")
```

## Fatias de Memória (Memory Slices)

Uma fatia é uma visão sobre múltiplos escopos. Útil para acesso somente leitura a vários ramos (ex.: escopo do agente + conhecimento da empresa):

```python
# Somente leitura: recall em /agent/researcher e /company/knowledge; remember() levanta erro
view = memory.slice(
    scopes=["/agent/researcher", "/company/knowledge"],
    read_only=True,
)
matches = view.recall("política da empresa sobre X", limit=10)

# Leitura e escrita: é preciso passar escopo explícito para remember()
view_rw = memory.slice(scopes=["/a", "/b"], read_only=False)
view_rw.remember("Novo fato", scope="/a", categories=[], importance=0.5)
```

## Camada de Análise LLM

A memória usa o LLM de três formas:

1. **Ao salvar** — Quando você omite escopo, categorias ou importância, o LLM analisa o conteúdo e sugere escopo, categorias, importância e metadados (entidades, datas, tópicos).
2. **Ao recall** — Para recall profundo/automático, o LLM analisa a consulta (palavras-chave, dicas de tempo, escopos sugeridos, complexidade) para guiar a recuperação.
3. **Extrair memórias** — `extract_memories(content)` quebra texto bruto (ex.: tarefa + resultado) em afirmações de memória discretas. Os agentes usam isso antes de chamar `remember()` em cada afirmação para que fatos atômicos sejam armazenados em vez de um único bloco grande.

## RecallFlow (Recall Profundo)

`recall()` suporta três profundidades:

- **`depth="shallow"`** — Busca vetorial direta com pontuação composta (semântica + recência + importância). Rápido; usado por padrão quando agentes carregam contexto para uma tarefa.
- **`depth="deep"`** ou **`depth="auto"`** — Executa um RecallFlow em múltiplas etapas: análise da consulta, seleção de escopo, busca vetorial, roteamento baseado em confiança e exploração recursiva opcional quando a confiança é baixa.

```python
# Caminho rápido (padrão para contexto de tarefa do agente)
matches = memory.recall("O que decidimos?", limit=10, depth="shallow")

# Caminho inteligente para perguntas complexas
matches = memory.recall("Resuma todas as decisões de arquitetura", limit=10, depth="auto")
```

## Pontuação Composta

A relevância é uma combinação ponderada de similaridade semântica, recência e importância. Configure os pesos via `MemoryConfig`:

```python
from crewai.memory import Memory, MemoryConfig

config = MemoryConfig(
    recency_weight=0.3,
    semantic_weight=0.5,
    importance_weight=0.2,
    recency_half_life_days=30,
)
memory = Memory(config=config)
```

## Descoberta

Inspecione a hierarquia de escopos e categorias:

```python
memory.tree()                    # Árvore formatada de escopos e contagem de registros
memory.tree("/project", max_depth=2)
memory.info("/project")         # ScopeInfo: record_count, categories, oldest/newest
memory.list_scopes("/")          # Escopos filhos imediatos
memory.list_categories()         # Nomes e contagens de categorias (global ou por escopo)
```

## Configuração de Embedder Customizado

Crews aceitam uma config `embedder` que é repassada ao Memory padrão quando `memory=True`:

```python
from crewai import Crew

# OpenAI (padrão)
crew = Crew(
    agents=[...],
    tasks=[...],
    memory=True,
    embedder={
        "provider": "openai",
        "config": {"model": "text-embedding-3-small"},
    },
)

# Ollama (local, privado)
crew = Crew(
    agents=[...],
    tasks=[...],
    memory=True,
    embedder={
        "provider": "ollama",
        "config": {"model": "mxbai-embed-large"},
    },
)
```

Memory standalone pode receber uma instância de embedder:

```python
from crewai.rag.embeddings.factory import build_embedder
from crewai import Memory

embedder = build_embedder({"provider": "openai", "config": {}})
memory = Memory(embedder=embedder)
```

| Provider       | Best For                       | Notes                    |
| -------------- | ------------------------------ | ------------------------- |
| **OpenAI**     | General use                    | Default, high quality     |
| **Ollama**     | Privacy, cost savings          | Local, no API key         |
| **Google AI**  | Google ecosystem               | gemini-embedding-001      |
| **Azure OpenAI** | Enterprise                   | Same config shape as OpenAI |
| **Cohere**     | Multilingual                   | embed-english-v3.0        |
| **VoyageAI**   | Retrieval tasks                | voyage-3                  |
| **Bedrock**    | AWS                            | amazon.titan-embed-text   |
| **Hugging Face** | Open-source models           | sentence-transformers     |

## Backend de Armazenamento

- **Padrão**: LanceDB, armazenado em `./.crewai/memory` (ou o caminho que você passar como `storage="path/to/dir"`).
- **Backend customizado**: Implemente o protocolo `StorageBackend` (veja `crewai.memory.storage.backend`) e passe uma instância para `Memory(storage=your_backend)`.

## Comportamento em Caso de Falha

Se o LLM falhar durante a análise (erro de rede, limite de taxa, resposta inválida), a memória degrada de forma graciosa:

- **Análise de save** — Um aviso é registrado e a memória ainda é armazenada com escopo padrão `/`, categorias vazias e importância `0.5`.
- **Extrair memórias** — O conteúdo completo é armazenado como uma única memória para que nada seja descartado.
- **Análise de consulta** — O recall usa fallback para seleção simples de escopo e busca vetorial, então você ainda obtém resultados.

Nenhuma exceção é levantada para essas falhas de análise; apenas falhas de armazenamento ou do embedder irão levantar.

## Nota sobre Privacidade

O conteúdo da memória é enviado ao LLM configurado para análise (escopo/categorias/importância no save, análise de consulta e recall profundo opcional). Para dados sensíveis, use um LLM local (ex.: Ollama) ou garanta que seu provedor atenda aos requisitos de conformidade.

## Eventos de Memória

Todas as operações de memória emitem eventos com `source_type="unified_memory"`. Você pode escutar para tempo, erros e conteúdo.

| Event | Description | Key Properties |
| :---- | :---------- | :------------- |
| **MemoryQueryStartedEvent** | Query begins | `query`, `limit` |
| **MemoryQueryCompletedEvent** | Query succeeds | `query`, `results`, `query_time_ms` |
| **MemoryQueryFailedEvent** | Query fails | `query`, `error` |
| **MemorySaveStartedEvent** | Save begins | `value`, `metadata` |
| **MemorySaveCompletedEvent** | Save succeeds | `value`, `save_time_ms` |
| **MemorySaveFailedEvent** | Save fails | `value`, `error` |
| **MemoryRetrievalStartedEvent** | Agent retrieval starts | `task_id` |
| **MemoryRetrievalCompletedEvent** | Agent retrieval done | `task_id`, `memory_content`, `retrieval_time_ms` |

Exemplo: monitorar tempo de consulta:

```python
from crewai.events import BaseEventListener, MemoryQueryCompletedEvent

class MemoryMonitor(BaseEventListener):
    def setup_listeners(self, crewai_event_bus):
        @crewai_event_bus.on(MemoryQueryCompletedEvent)
        def on_done(source, event):
            if getattr(event, "source_type", None) == "unified_memory":
                print(f"Query '{event.query}' completed in {event.query_time_ms:.0f}ms")
```

## Solução de Problemas

**Memória não persiste?**
- Garanta que o caminho de armazenamento seja gravável (padrão `./.crewai/memory`). Passe `storage="./your_path"` para usar outro diretório.
- Ao usar uma crew, confirme que `memory=True` ou `memory_instance=...` está definido.

**Recall lento?**
- Use `depth="shallow"` para contexto rotineiro do agente. Reserve `depth="auto"` ou `"deep"` para consultas complexas.

**Erros de análise LLM nos logs?**
- A memória ainda salva/recupera com padrões seguros. Verifique chaves de API, limites de taxa e disponibilidade do modelo se quiser análise LLM completa.

**Resetar memória (ex.: para testes):**
```python
crew.reset_memories(command_type="memory")  # Reseta memória unificada
# Ou em uma instância Memory:
memory.reset()           # Todos os escopos
memory.reset(scope="/project/old")  # Apenas essa subárvore
```

## Benefícios

- **API única** — Uma classe `Memory` para todos os casos de uso; sem tipos separados short/long/entity/external.
- **Organizada por LLM** — Escopo e categorias inferidos do conteúdo; extração atômica opcional para recall mais limpo.
- **Recall adaptativo** — Caminho shallow para velocidade, caminho deep para perguntas complexas com roteamento baseado em confiança.
- **Visões composáveis** — Escopos e fatias para acesso por agente ou multi-escopo sem fragmentar o armazenamento.
- **Observável** — Eventos para cada save e query para monitorar e depurar.
