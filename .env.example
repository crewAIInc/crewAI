# ========================================
# CrewAI Configuration for Local LLMs
# Windows 11 + Ollama Setup
# ========================================

# --------------------------------------
# LOCAL OLLAMA CONFIGURATION
# --------------------------------------
# No API keys needed for local models!

# Default model for all agents (if not specified in agent config)
# Recommended MoE models for GTX 5080:
#   - ollama/qwen2.5:32b (Best reasoning, recommended)
#   - ollama/deepseek-r1:14b (Fast reasoning)
#   - ollama/deepseek-r1:32b (Powerful reasoning)
#   - ollama/phi4:14b (Microsoft, balanced)
MODEL=ollama/qwen2.5:32b

# Ollama API endpoint (default: http://localhost:11434)
OLLAMA_API_BASE=http://localhost:11434

# Optional: Specify model name without provider prefix
# This will be used by agents that don't specify a model
# OPENAI_MODEL_NAME=qwen2.5:32b

# --------------------------------------
# ADVANCED OLLAMA SETTINGS
# --------------------------------------

# Temperature (0.0-1.0, higher = more creative)
# LLM_TEMPERATURE=0.7

# Max tokens for responses
# LLM_MAX_TOKENS=4096

# Request timeout (seconds)
# LLM_TIMEOUT=120

# --------------------------------------
# CLOUD PROVIDERS (OPTIONAL FALLBACK)
# --------------------------------------
# Uncomment to enable cloud models as fallback

# OpenAI (GPT-4, GPT-4o, etc.)
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL_NAME=gpt-4o-mini

# Anthropic Claude
# ANTHROPIC_API_KEY=sk-ant-...

# Google Gemini
# GEMINI_API_KEY=...

# Groq (Fast inference)
# GROQ_API_KEY=...

# --------------------------------------
# CREWAI SETTINGS
# --------------------------------------

# Enable debug logging (0=off, 1=on)
CREWAI_DEBUG=0

# Telemetry (set to false to disable)
# CREWAI_TELEMETRY_OPT_OUT=true

# Max iterations for agents
# MAX_ITER=25

# Max RPM (requests per minute) - not applicable for local models
# MAX_RPM=60

# --------------------------------------
# EMBEDDING MODELS (FOR RAG/KNOWLEDGE)
# --------------------------------------
# Ollama can also provide embeddings for semantic search

# Use Ollama for embeddings (recommended for privacy)
# EMBEDDINGS_MODEL=ollama/nomic-embed-text
# EMBEDDINGS_BASE_URL=http://localhost:11434

# Or use OpenAI embeddings
# OPENAI_API_KEY=sk-...
# EMBEDDINGS_MODEL=text-embedding-3-small

# --------------------------------------
# MEMORY CONFIGURATION
# --------------------------------------

# Enable memory (short-term, long-term, entity)
# MEMORY_ENABLED=true

# Memory provider (local, redis, etc.)
# MEMORY_PROVIDER=local

# --------------------------------------
# TOOL CONFIGURATION
# --------------------------------------

# Search API keys (for web search tools)
# SERPER_API_KEY=...
# BROWSERLESS_API_KEY=...

# --------------------------------------
# PERFORMANCE TUNING
# --------------------------------------

# Number of concurrent agent tasks
# MAX_CONCURRENT_TASKS=4

# GPU memory fraction (if using multiple models)
# CUDA_VISIBLE_DEVICES=0

# --------------------------------------
# WINDOWS-SPECIFIC
# --------------------------------------

# Set Python to use UTF-8 encoding
PYTHONIOENCODING=utf-8

# Disable Windows-specific warnings
PYTHONWARNINGS=ignore

# ========================================
# QUICK START GUIDE
# ========================================
#
# 1. Copy this file to .env
# 2. Update MODEL to your preferred Ollama model
# 3. Make sure Ollama is running: ollama serve
# 4. Run your crew!
#
# Test Ollama connection:
#   curl http://localhost:11434/api/version
#
# List installed models:
#   ollama list
#
# Pull a new model:
#   ollama pull qwen2.5:32b
#
# ========================================
